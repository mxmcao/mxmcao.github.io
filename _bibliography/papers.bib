---
---

% =============================================================================
% FIELD REFERENCE — rendered by _layouts/bib.liquid
% =============================================================================
%
% ── VISUAL (left column) ────────────────────────────────────────────────────
%   abbr        Venue badge, e.g. {NeurIPS}
%               → Color/URL configured in _data/venues.yml
%               → bib.liquid lines 6–26
%
%   preview     Thumbnail filename, e.g. {my-paper.png}
%               → File must be placed in assets/img/publication_preview/
%               → bib.liquid lines 28–43
%
% ── BUTTONS (PDF · Code · Bib) ───────────────────────────────────────────────
%   arxiv       arXiv ID, e.g. {2508.09874}  →  renders "PDF" button
%               → bib.liquid line 192–194
%
%   website     Full URL, e.g. {https://github.com/...}  →  renders "Code" button
%               → bib.liquid line 195–197
%
%   bibtex_show Set {true} to show the "Bib" button (expands raw BibTeX)
%               → bib.liquid line 198–200
%
% ── BADGES (Stars · Forks · Scholar · HuggingFace · Altmetric) ───────────────
%   github      Repo path, e.g. {LUMIA-Group/MemoryDecoder}
%               → Renders GitHub Stars badge via shields.io
%               → bib.liquid lines 225–237
%
%   github_show_forks  Set {true} to also show the GitHub Forks badge (optional)
%               → Default: not shown  →  set {true} to enable
%
%   google_scholar_id  Paper-specific Scholar ID, e.g. {qyhmnyLat1gC}
%               → HOW TO GET IT:
%                    1. Go to your Google Scholar profile and click the paper
%                    2. The URL looks like:
%                       scholar.google.com/...&citation_for_view=USERID:PAPERID
%                    3. Copy the PAPERID after the colon — that is your ID
%                    4. Also set scholar_userid in _data/socials.yml
%               → bib.liquid lines 239–265
%
%   huggingface Full HuggingFace URL, e.g. {https://huggingface.co/papers/...}
%               → Renders HuggingFace badge
%               → bib.liquid lines 267–273
%
%   huggingface_name  Custom label shown on the HuggingFace badge (optional)
%               → Default: "HuggingFace"  →  e.g. {Memory Decoder} or {MLP Memory}
%               → bib.liquid lines 267–273
%
%   altmetric   Set {true} to auto-detect via arxiv ID, or provide explicit ID
%               → bib.liquid lines 275–293
%
% ── METADATA ─────────────────────────────────────────────────────────────────
%   title       Full paper title
%   author      "First Last and First Last and ..." — use * suffix for equal
%               contribution, e.g. {Jiaqi Cao* and Rubin Wei*}
%               The * becomes a superscript; explain it in annotation = {* Equal contribution}
%               Your name is auto-bolded — configured via scholar.last_name /
%               scholar.first_name in _config.yml (currently: Jiaqi Cao)
%   booktitle   Conference full name (for @inproceedings)
%   journal     Journal name (for @article)
%   year        Publication year
%
% ── OTHER ────────────────────────────────────────────────────────────────────
%   abstract    Shown in the hidden "Abs" block (kept for reference even though
%               the Abs button is currently removed from the UI)
%   annotation  Footnote shown as a popover icon next to the author list
%   selected    Set {true} to show this paper on the homepage (about page)
%
% =============================================================================

@inproceedings{cao2025memorydecoder,
  abbr        = {NeurIPS},
  bibtex_show = {true},
  title       = {Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models},
  author      = {Jiaqi Cao* and Jiarui Wang* and Rubin Wei and Qipeng Guo and Kai Chen and Bowen Zhou and Zhouhan Lin},
  booktitle   = {Advances in Neural Information Processing Systems},
  year        = {2025},
  arxiv       = {2508.09874},
  website     = {https://github.com/LUMIA-Group/MemoryDecoder},
  github      = {LUMIA-Group/MemoryDecoder},
  huggingface = {https://huggingface.co/collections/Clover-Hill/memorydecoder},
  huggingface_name = {Memory Decoder},
  google_scholar_id = {d1gkVwhDpl0C},
  abstract    = {Large Language Models (LLMs) excel at general language tasks but struggle with domain adaptation. Domain Adaptive Pretraining (DAPT) is costly and suffers from catastrophic forgetting, while Retrieval-Augmented Generation (RAG) introduces substantial inference latency. We propose Memory Decoder, a pretrained, plug-and-play memory module that enables efficient domain adaptation without modifying the original model's parameters.},
  preview     = {memory-decoder.png},
  altmetric = {false},
  selected    = {true}
}

@inproceedings{wei2026mlpmemory,
  abbr        = {ICLR},
  bibtex_show = {true},
  title       = {MLP Memory: A Retriever-Pretrained Memory for Large Language Models},
  author      = {Rubin Wei* and Jiaqi Cao* and Jiarui Wang and Jushi Kai and Qipeng Guo and Bowen Zhou and Zhouhan Lin},
  booktitle   = {International Conference on Learning Representations},
  year        = {2026},
  arxiv       = {2508.01832},
  website     = {https://github.com/Rubin-Wei/MLPMemory},
  github      = {Rubin-Wei/MLPMemory},
  huggingface = {https://huggingface.co/collections/Rubin-Wei/mlpmemory},
  huggingface_name = {MLP Memory},
  google_scholar_id = {9yKSN-GCB0IC},
  abstract    = {We present MLP Memory, a lightweight parametric module that pretrains an MLP to imitate a kNN retriever's behavior on the entire pretraining dataset. This creates a differentiable memory component that internalizes retrieval patterns without explicit document access, achieving 17.5\% and 24.1\% scaling gains on WikiText-103 and Web datasets, respectively.},
  preview     = {mlp-memory.png},
  altmetric = {false},
  selected    = {true}
}
